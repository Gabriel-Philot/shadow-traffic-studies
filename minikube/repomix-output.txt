This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
access-control/
  crb-jupyter.yaml
  crb-spok.yaml
images_docker/
  datagenShadow/
    Dockerfile
    license.env
  jupyter-spark/
    Dockerfile
  spok-plus-modules/
    utils/
      utils_test.py
    Dockerfile
    download-jars.sh
manifests/
  datagen/
    generator-secrets.yaml
    shadowtraffic-config.yaml
    shadowtraffic-deployment.yaml
  deepstorage/
    minio.yaml
  jupyter-spark/
    jupyter-app.yaml
  management/
    reflector.yaml
  misc/
    access-control.yaml
    secrets.yaml
  processing/
    spark-operator.yaml
scripts-bash/
  automation_config_k8.sh
  test_minio_connection.sh
  upload-script-spark.sh
secrets/
  minio-secrets.yaml
spark-jobs/
  spark-job-modules-test.yaml
src/
  test_spark2.py

================================================================
Files
================================================================

================
File: access-control/crb-jupyter.yaml
================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-jupyter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: default  
    namespace: jupyter

================
File: access-control/crb-spok.yaml
================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-spok
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: default  
    namespace: processing

================
File: images_docker/datagenShadow/Dockerfile
================
# Usar a imagem base do ShadowTraffic
FROM shadowtraffic/shadowtraffic:latest

# Instrução para usar variáveis de ambiente no ShadowTraffic
ENV LICENSE_ID=$LICENSE_ID
ENV LICENSE_EMAIL=$LICENSE_EMAIL
ENV LICENSE_ORGANIZATION=$LICENSE_ORGANIZATION
ENV LICENSE_EDITION=$LICENSE_EDITION
ENV LICENSE_EXPIRATION=$LICENSE_EXPIRATION
ENV LICENSE_SIGNATURE=$LICENSE_SIGNATURE
ENV AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
ENV AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
ENV AWS_REGION=$AWS_REGION

# Comando de entrada, garantindo que as variáveis sejam usadas
CMD ["shadowtraffic", "--config", "/workspace/v0_local_gen_minio.json"]

================
File: images_docker/datagenShadow/license.env
================
LICENSE_ID=cb9552fb-2902-49ce-b178-61190a602079
LICENSE_EMAIL=gabriel2024case@gmail.com
LICENSE_ORGANIZATION=gordostest
LICENSE_EDITION=ShadowTraffic Free Trial
LICENSE_EXPIRATION=2025-03-23
LICENSE_SIGNATURE=cI+afvP4GPbNgN95/rGY/lHNlyICrctVviPbP7AtqVpuwXufaNX7lg8cPfbgnsd6ljuUfRceGeYLANF7L5pJj1kQFc/mQhAAowxbXjhUCRcjh9T8iqW7P9Fc6AHHbCDJ0/niEQ2Qvl/98Knke13CvoFCzPnEZHCQVh9Sy5EcpFA8T9j5LYGbv+YxdKYxxT68aPL9lRSZPeLFwEVjY83YCoM4hlYyq9FTrYCKQwIE0UoJjhY8Uy1Gt5Iqj0z4MoFMyTOAph8AmA0iZdckTEZO6dxUu5HrFmuL4Y9EIUqjydFXYzqwtvvaH7i5w4fSCdJhhhDRM94cD5JIw+B5WykBug==

================
File: images_docker/jupyter-spark/Dockerfile
================
FROM jupyter/pyspark-notebook:latest

USER root

# Install additional Python packages
RUN pip install --no-cache-dir \
    delta-spark \
    boto3 \
    s3fs \
    matplotlib \
    seaborn \
    pandas \
    pyarrow

# Create directories for resources and configs
RUN mkdir -p /home/jovyan/resources /home/jovyan/config /home/jovyan/work && \
    chown -R jovyan:users /home/jovyan/resources /home/jovyan/config /home/jovyan/work

# Switch back to jovyan user
USER jovyan

# Set environment variables for Spark
ENV PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.1.0,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell"

# Define the startup command for JupyterLab - simpler like your original
CMD ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser"]

================
File: images_docker/spok-plus-modules/utils/utils_test.py
================
def hello_spark():
    """Retorna uma mensagem de teste."""
    return "Hello from utils_test.py! Módulo importado com sucesso!"

def process_data(df):
    """
    Processa um DataFrame adicionando novas colunas.
    
    Args:
        df: Um DataFrame Spark
    
    Returns:
        DataFrame processado com novas colunas
    """
    from pyspark.sql import functions as F
    
    # Adiciona uma coluna com a idade em meses
    processed = df.withColumn("idade_meses", F.col("idade") * 12)
    
    # Adiciona uma coluna com a categoria de idade
    processed = processed.withColumn(
        "categoria_idade",
        F.when(F.col("idade") < 25, "jovem")
         .when(F.col("idade") < 35, "adulto")
         .otherwise("senior")
    )
    
    return processed

================
File: images_docker/spok-plus-modules/Dockerfile
================
FROM spark:3.5.3

USER root

RUN chmod -R 777 /tmp

# Criar diretório para módulos Python personalizados
RUN mkdir -p /opt/spark/work-dir/utils

# Copiar módulos Python auxiliares
COPY utils/ /opt/spark/work-dir/utils/

# Copiar JARs para o diretório jars do Spark
COPY jars/*.jar $SPARK_HOME/jars/

# Definir permissões corretas
RUN chmod 645 $SPARK_HOME/jars/*.jar && \
    chmod -R 777 /opt/spark/work-dir/utils

# Instalar bibliotecas Python
RUN pip install delta-spark==3.3.0
RUN pip install boto3

# Voltar para o usuário Spark
USER ${spark_uid}

================
File: images_docker/spok-plus-modules/download-jars.sh
================
#!/bin/bash

# Cores para melhor visualização
GREEN='\033[0;32m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== Script para download de JARs do Spark ===${NC}"

# Criar diretório jars se não existir
if [ ! -d "jars" ]; then
    echo -e "Criando diretório jars..."
    mkdir -p jars
    echo -e "${GREEN}✓ Diretório jars criado com sucesso!${NC}"
else
    echo -e "${GREEN}✓ Diretório jars já existe!${NC}"
fi

# Função para baixar JARs
download_jar() {
    local url=$1
    local filename=$(basename "$url")
    local target="jars/$filename"
    
    # Verifica se o arquivo já existe
    if [ -f "$target" ]; then
        echo -e "${GREEN}✓ Arquivo $filename já existe. Pulando...${NC}"
        return 0
    fi
    
    echo -e "Baixando $filename..."
    wget -q --show-progress "$url" -O "$target"
    
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}✓ Download de $filename concluído com sucesso!${NC}"
    else
        echo -e "${RED}✗ Falha ao baixar $filename!${NC}"
        return 1
    fi
}

# Lista de JARs para baixar
echo -e "\n${BLUE}Iniciando downloads:${NC}"

# Hadoop/AWS JARs
download_jar "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
download_jar "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.4/hadoop-common-3.3.4.jar"
download_jar "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar"

# Delta Lake JARs
download_jar "https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.0/delta-spark_2.12-3.3.0.jar"
download_jar "https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.0/delta-storage-3.3.0.jar"

# JARs adicionais que podem ser úteis
download_jar "https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar"

echo -e "\n${BLUE}=== Resumo ===${NC}"
echo -e "Total de JARs no diretório: $(ls jars | wc -l)"
echo -e "${GREEN}Download de JARs concluído!${NC}"
echo -e "Os JARs estão disponíveis no diretório: $(pwd)/jars"

================
File: manifests/datagen/generator-secrets.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: generator-secrets
  namespace: datagen
  annotations:
    reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "deepstorage,datagen"
type: Opaque
stringData:
  LICENSE_ID: "cb9552fb-2902-49ce-b178-61190a602079"
  LICENSE_EMAIL: "gabriel2024case@gmail.com"
  LICENSE_ORGANIZATION: "gordostest"
  LICENSE_EDITION: "ShadowTraffic Free Trial"
  LICENSE_EXPIRATION: "2025-03-23"
  LICENSE_SIGNATURE: "cI+afvP4GPbNgN95/rGY/lHNlyICrctVviPbP7AtqVpuwXufaNX7lg8cPfbgnsd6ljuUfRceGeYLANF7L5pJj1kQFc/mQhAAowxbXjhUCRcjh9T8iqW7P9Fc6AHHbCDJ0/niEQ2Qvl/98Knke13CvoFCzPnEZHCQVh9Sy5EcpFA8T9j5LYGbv+YxdKYxxT68aPL9lRSZPeLFwEVjY83YCoM4hlYyq9FTrYCKQwIE0UoJjhY8Uy1Gt5Iqj0z4MoFMyTOAph8AmA0iZdckTEZO6dxUu5HrFmuL4Y9EIUqjydFXYzqwtvvaH7i5w4fSCdJhhhDRM94cD5JIw+B5WykBug=="

================
File: manifests/datagen/shadowtraffic-config.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadowtraffic-config
  namespace: datagen
data:
  v0_local_gen_minio.json: |
    {
      "generators": [
        {
          "connection": "s3",
          "bucket": "landing",
          "bucketConfigs": {
            "keyPrefix": "shadow-traffic-data/customers/",
            "format": "json"
          },
          "fork": {
            "maxForks": 20,
            "key": { "_gen": "uuid" }
          },
          "data": {
            "id": { "_gen": "var", "var": "forkKey" },
            "profile": {
              "first_name": { "_gen": "string", "expr": "#{Name.firstName}" },
              "last_name": { "_gen": "string", "expr": "#{Name.lastName}" },
              "full_name": { "_gen": "string", "expr": "#{Name.fullName}" },
              "age": { "_gen": "uniformDistribution", "bounds": [18, 80], "decimals": 0 },
              "email": { "_gen": "string", "expr": "#{Internet.emailAddress}" },
              "avatar": { "_gen": "string", "expr": "https://cloudflare-ipfs.com/ipfs/Qmd3W5DuhgHirLHGVixi6V76LhCkZUz6pnFt5AJBiyvHye/avatar/#{number.numberBetween '1','1000'}.jpg" }
            },
            "address": {
              "street": { "_gen": "string", "expr": "#{Address.streetAddress}" },
              "city": { "_gen": "string", "expr": "#{Address.city}" },
              "state": { "_gen": "string", "expr": "#{Address.stateAbbr}" },
              "zipcode": { "_gen": "string", "expr": "#{Address.zipCode}" },
              "country": { "_gen": "string", "expr": "#{Address.country}" },
              "coordinates": {
                "latitude": { "_gen": "uniformDistribution", "bounds": [-90, 90], "decimals": 6 },
                "longitude": { "_gen": "uniformDistribution", "bounds": [-180, 180], "decimals": 6 }
              }
            }
          },
          "localConfigs": {
            "maxEvents": 5,
            "throttleMs": 1000
          }
        }
      ],
      "connections": {
        "s3": {
          "kind": "s3",
          "connectionConfigs": {
            "endpoint": "http://minio.deepstorage.svc.cluster.local:9000",
            "pathStyleAccess": true,
            "forcePathStyle": true
          }
        }
      }
    }

================
File: manifests/datagen/shadowtraffic-deployment.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shadowtraffic
  namespace: datagen
spec:
  replicas: 1
  selector:
    matchLabels:
      app: shadowtraffic
  template:
    metadata:
      labels:
        app: shadowtraffic
    spec:
      containers:
        - name: shadowtraffic
          image: shadowtraffic-datagen:latest
          imagePullPolicy: Never
          env:
            - name: LICENSE_ID
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_ID
            - name: LICENSE_EMAIL
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EMAIL
            - name: LICENSE_ORGANIZATION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_ORGANIZATION
            - name: LICENSE_EDITION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EDITION
            - name: LICENSE_EXPIRATION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EXPIRATION
            - name: LICENSE_SIGNATURE
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_SIGNATURE
            - name: AWS_ACCESS_KEY_ID
              value: "miniouser"
            - name: AWS_SECRET_ACCESS_KEY
              value: "miniosecret" 
            - name: AWS_REGION
              value: "us-east-1"
          volumeMounts:
            - name: config-volume
              mountPath: /workspace/v0_local_gen_minio.json
              subPath: v0_local_gen_minio.json
      volumes:
        - name: config-volume
          configMap:
            name: shadowtraffic-config

================
File: manifests/deepstorage/minio.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: minio
  annotations:
    argocd.argoproj.io/sync-wave: "6"
  namespace: cicd
spec:
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: deepstorage
  project: default
  source:
    repoURL: 'https://charts.bitnami.com/bitnami'
    targetRevision: 14.0.0
    helm:
      values: |-
        auth:
          existingSecret: minio-secrets
        defaultBuckets: "landing, lakehouse, scripts"
        service:
          type: LoadBalancer
        persistence:
          size: 10Gi
        volumePermissions:
          enabled: true
    chart: minio
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/jupyter-spark/jupyter-app.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: custom-jupyter
  namespace: cicd
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: jupyter
  source:
    repoURL: git@github.com:Gabriel-Philot/shadow-traffic-studies.git
    targetRevision: HEAD
    path: minikube/manifests/jupyter-spark
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
---
apiVersion: v1
kind: Service
metadata:
  name: custom-jupyter
  namespace: jupyter
spec:
  type: LoadBalancer
  ports:
  - port: 8888
    targetPort: 8888
    protocol: TCP
  selector:
    app: custom-jupyter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-jupyter
  namespace: jupyter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: custom-jupyter
  template:
    metadata:
      labels:
        app: custom-jupyter
    spec:
      containers:
      - name: jupyter
        image: jupyter:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 8888
        env:
        - name: MINIO_ENDPOINT
          value: "http://minio.deepstorage.svc.cluster.local:9000"
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secrets
              key: root-user
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secrets
              key: root-password
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
          requests:
            memory: "2Gi"
            cpu: "1"

================
File: manifests/management/reflector.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: reflector
  annotations:
    argocd.argoproj.io/sync-wave: "5"
  namespace: cicd
spec:
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: management
  project: default
  source:
    repoURL: 'https://emberstack.github.io/helm-charts'
    targetRevision: 7.0.151        
    chart: reflector
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/misc/access-control.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: access-control
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: misc
  project: default
  source:
    repoURL: "git@github.com:Gabriel-Philot/shadow-traffic-studies.git"
    path: minikube/access-control/
    targetRevision: HEAD
    directory:
      recurse: true
      jsonnet: {}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/misc/secrets.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: secrets
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: misc
  project: default
  source:
    repoURL: "git@github.com:Gabriel-Philot/shadow-traffic-studies.git"
    path: minikube/secrets/
    targetRevision: HEAD
    directory:
      recurse: true
      jsonnet: {}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/processing/spark-operator.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: spark
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: processing
  project: default
  source:
    repoURL: "https://kubeflow.github.io/spark-operator"
    targetRevision: 1.4.2
    helm:
      values: |-
        webhook:
          enable: true
    chart: spark-operator
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: scripts-bash/automation_config_k8.sh
================
#!/bin/bash

# Exit on error
set -e

# Color codes for better output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
print_section() {
    echo -e "${BLUE}===================================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}===================================================${NC}"
}

print_success() {
    echo -e "${GREEN}✓ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}

print_error() {
    echo -e "${RED}✗ $1${NC}"
}

wait_for_pod_ready() {
    namespace=$1
    label=$2
    echo -e "Waiting for pods with label $label in namespace $namespace to be ready..."
    
    while [[ $(kubectl get pods -n $namespace -l $label -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do
        echo -n "."
        sleep 3
    done
    echo
    print_success "Pod is ready!"
}

wait_for_loadbalancer_ip() {
    namespace=$1
    label=$2
    echo -e "Waiting for LoadBalancer IP for service with label $label in namespace $namespace..."
    
    while [[ -z $(kubectl get services -n $namespace -l $label -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}") ]]; do
        echo -n "."
        sleep 3
    done
    echo
    print_success "LoadBalancer IP is ready!"
}

# Assuming minikube is already running
print_success "Minikube is assumed to be running with tunnel active"

print_section "Creating necessary namespaces"
kubectl create namespace orchestrator 2>/dev/null || echo "Namespace orchestrator already exists"
kubectl create namespace deepstorage 2>/dev/null || echo "Namespace deepstorage already exists"
kubectl create namespace cicd 2>/dev/null || echo "Namespace cicd already exists"
kubectl create namespace app 2>/dev/null || echo "Namespace app already exists"
kubectl create namespace management 2>/dev/null || echo "Namespace management already exists"
kubectl create namespace misc 2>/dev/null || echo "Namespace misc already exists"
kubectl create namespace jupyter 2>/dev/null || echo "Namespace jupyter already exists"
kubectl create namespace processing 2>/dev/null || echo "Namespace jupyter already exists"
kubectl create namespace datagen 2>/dev/null || echo "Namespace datagen already exists"

print_section "Installing ArgoCD"
echo "Installing ArgoCD with Helm..."
helm install argocd argo/argo-cd --namespace cicd --version 5.27.1 || echo "ArgoCD already installed or error occurred"

echo "Patching ArgoCD server service to use LoadBalancer..."
kubectl patch svc argocd-server -n cicd -p '{"spec": {"type": "LoadBalancer"}}'

print_section "Waiting for ArgoCD to be ready"
wait_for_pod_ready "cicd" "app.kubernetes.io/name=argocd-server"
wait_for_loadbalancer_ip "cicd" "app.kubernetes.io/name=argocd-server"

# Store ArgoCD LoadBalancer IP
ARGOCD_LB=$(kubectl get services -n cicd -l app.kubernetes.io/name=argocd-server -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}")
print_success "ArgoCD LoadBalancer IP: $ARGOCD_LB"

# Get ArgoCD admin password
ARGOCD_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -n cicd -o jsonpath="{.data.password}" | base64 -d)
print_success "ArgoCD admin password: $ARGOCD_PASSWORD"

echo "ArgoCD web interface available at: http://$ARGOCD_LB"

# Assuming minikube tunnel is already running
print_success "Minikube tunnel is assumed to be running in another terminal"

print_section "Adding Git repository to ArgoCD"
echo "Logging into ArgoCD..."
kubectl get secret argocd-initial-admin-secret -n cicd -o jsonpath="{.data.password}" | base64 -d | xargs -t -I {} argocd login $ARGOCD_LB --username admin --password {} --insecure

echo "Adding repository to ArgoCD..."
argocd repo add git@github.com:Gabriel-Philot/shadow-traffic-studies.git --ssh-private-key-path ~/.ssh/id_ed25519 --insecure-skip-server-verification

print_success "Repository added to ArgoCD"

print_section "Setting up Reflector for secret management"
kubectl apply -f manifests/management/reflector.yaml

print_warning "Waiting for Reflector to be deployed (30 seconds)..."
sleep 30

print_section "Setting up Secrets"
kubectl apply -f manifests/misc/secrets.yaml

print_warning "Waiting for Secrets to be processed (10 seconds)..."
sleep 10

print_section "Setting up Acess control"
kubectl apply -f access-control/crb-jupyter.yaml

print_warning "Waiting for Acess control to be processed (10 seconds)..."
sleep 10

print_section "Setting up MinIO Storage"
kubectl apply -f manifests/deepstorage/minio.yaml

print_warning "Waiting for MinIO to be deployed (45 seconds)..."
sleep 45

print_section "Building and deploying ShadowTraffic data generator"

# Switch to minikube's Docker environment
eval $(minikube docker-env)

echo "Building ShadowTraffic Docker image..."
docker build --no-cache -f images_docker/datagenShadow/Dockerfile -t shadowtraffic-datagen:latest .

echo "Deploying ShadowTraffic components..."
kubectl apply -f manifests/datagen/generator-secrets.yaml
kubectl apply -f manifests/datagen/shadowtraffic-config.yaml
kubectl apply -f manifests/datagen/shadowtraffic-deployment.yaml

wait_for_pod_ready "datagen" "app=shadowtraffic"

print_section "Getting access credentials"

# Get MinIO LoadBalancer IP
MINIO_LB=$(kubectl get services -n deepstorage -l app.kubernetes.io/name=minio -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}")
MINIO_USER=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-user}" | base64 -d)
MINIO_PASSWORD=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-password}" | base64 -d)

print_success "Setup Complete!"
echo "================ SERVICE ACCESS INFORMATION ================"
echo "ArgoCD:"
echo "  URL: http://$ARGOCD_LB"
echo "  Username: admin"
echo "  Password: $ARGOCD_PASSWORD"
echo ""
echo "MinIO:"
echo "  URL: http://$MINIO_LB:9000"
echo "  Username: $MINIO_USER"
echo "  Password: $MINIO_PASSWORD"

print_success "Git repository has been configured in ArgoCD"

echo "To check the ShadowTraffic generator status, run:"
echo "kubectl get pods -n datagen"

# Return to the original Docker environment
eval $(minikube docker-env -u)

================
File: scripts-bash/test_minio_connection.sh
================
#!/bin/bash

# Install MinIO client if not already installed
if ! command -v mc &> /dev/null; then
    echo "Installing MinIO client..."
    wget https://dl.min.io/client/mc/release/linux-amd64/mc
    chmod +x mc
    sudo mv mc /usr/local/bin/
fi

# Configure MinIO client
mc alias set minikube-minio http://10.108.217.62:9000 miniouser miniosecret

# List buckets to verify connection
echo "Testing connection to MinIO..."
mc ls minikube-minio

# Check if landing bucket exists, create if it doesn't
if ! mc ls minikube-minio/landing &> /dev/null; then
    echo "Creating landing bucket..."
    mc mb minikube-minio/landing
fi

echo "Connection test complete!"

================
File: scripts-bash/upload-script-spark.sh
================
#!/bin/bash

# Cores para saída formatada
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
NC='\033[0m' # Sem cor

echo -e "${YELLOW}Iniciando upload do script para o MinIO...${NC}"

# Definir variáveis
MINIO_ENDPOINT=$(kubectl get services -n deepstorage -l app.kubernetes.io/name=minio -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}"):9000
MINIO_USER=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-user}" | base64 -d)
MINIO_PASSWORD=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-password}" | base64 -d)

echo -e "MinIO endpoint: ${MINIO_ENDPOINT}"
echo -e "MinIO user: ${MINIO_USER}"

# Verificar se a AWS CLI está instalada
if ! command -v aws &> /dev/null; then
    echo -e "${YELLOW}AWS CLI não encontrada. Instalando...${NC}"
    pip install awscli
fi

# Configurar AWS CLI para MinIO
echo -e "Configurando AWS CLI para acessar MinIO..."
mkdir -p ~/.aws

cat > ~/.aws/credentials << EOF
[default]
aws_access_key_id = ${MINIO_USER}
aws_secret_access_key = ${MINIO_PASSWORD}
EOF

cat > ~/.aws/config << EOF
[default]
region = us-east-1
EOF

# Verificar se o bucket scripts existe
echo -e "Verificando se o bucket 'scripts' existe..."
if ! aws --endpoint-url=http://${MINIO_ENDPOINT} s3 ls s3://scripts &> /dev/null; then
    echo -e "Bucket 'scripts' não existe. Criando..."
    aws --endpoint-url=http://${MINIO_ENDPOINT} s3 mb s3://scripts
    echo -e "${GREEN}Bucket 'scripts' criado com sucesso!${NC}"
else
    echo -e "${GREEN}Bucket 'scripts' já existe.${NC}"
fi

# Fazer upload do script de teste
echo -e "\n${YELLOW}Fazendo upload do script de teste para o MinIO...${NC}"
aws --endpoint-url=http://${MINIO_ENDPOINT} s3 cp src/test_spark2.py s3://scripts/minio-spark-scripts/

if [ $? -eq 0 ]; then
    echo -e "${GREEN}Script enviado para o MinIO com sucesso!${NC}"
    echo -e "Caminho no MinIO: s3a://scripts/minio-spark-scripts/test_spark2.py"
else
    echo -e "${RED}Erro ao enviar script para o MinIO!${NC}"
fi

echo -e "\n${GREEN}Processo de upload concluído!${NC}"

================
File: secrets/minio-secrets.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: minio-secrets
  namespace: deepstorage
  annotations:
    reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "deepstorage,datagen,jupyter, processing"
type: Opaque
data:
  root-user: "bWluaW91c2Vy"
  root-password: "bWluaW9zZWNyZXQ="

================
File: spark-jobs/spark-job-modules-test.yaml
================
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: minio-spark-teste
  namespace: processing
  labels:
    app: spark
    product: data-processing
    source: minio
    table: teste
    stage: dev
spec:
  type: Python
  mode: cluster
  image: custom-spark:latest
  imagePullPolicy: Never
  mainApplicationFile: "s3a://scripts/minio-spark-scripts/test_spark2.py"
  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
  driver:
    cores: 1
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  executor:
    cores: 1
    instances: 2
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  sparkConf:
    "spark.hadoop.fs.s3a.endpoint": "http://minio.deepstorage.svc.cluster.local:9000"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.delta.logStore.class": "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore"
    "spark.databricks.delta.schema.autoMerge.enabled": "true"
    "spark.databricks.delta.retentionDurationCheck.enabled": "false"
    "spark.jars.ivy": "/tmp/.ivy2"
    "spark.executorEnv.PYTHONPATH": "/opt/spark/work-dir"
    "spark.driver.extraClassPath": "/opt/spark/work-dir"

================
File: src/test_spark2.py
================
"""
Script de teste para verificar o acesso direto ao MinIO via S3A
"""
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os
import sys
import socket
import datetime

# Diagnóstico e configuração do ambiente
print("="*80)
print("DIAGNÓSTICO DO AMBIENTE PYTHON")
print(f"Data/Hora atual: {datetime.datetime.now()}")
print(f"Hostname: {socket.gethostname()}")
print(f"Diretório atual: {os.getcwd()}")
print(f"Conteúdo do diretório atual: {os.listdir('.')}")

# Adicionar caminhos ao sys.path
possible_paths = [
    '/opt/spark/work-dir',
    '/opt/spark/work-dir/utils',
    '/usr/local/lib/python3.9/site-packages',
    '/usr/local/lib/python3.8/site-packages',
    '/usr/local/lib/python3.7/site-packages',
    os.getcwd()
]

for path in possible_paths:
    if path not in sys.path and os.path.exists(path):
        sys.path.insert(0, path)
        print(f"Adicionado ao sys.path: {path}")

print(f"PYTHONPATH atual: {sys.path}")

# Verificar diretórios onde o módulo utils poderia estar
for path in sys.path:
    try:
        if os.path.exists(path):
            print(f"Conteúdo de {path}: {os.listdir(path)}")
            if os.path.exists(os.path.join(path, 'utils')):
                utils_dir = os.path.join(path, 'utils')
                print(f"Conteúdo de {utils_dir}: {os.listdir(utils_dir)}")
    except Exception as e:
        print(f"Erro ao listar {path}: {e}")

# Tentar importar de diferentes maneiras
try:
    # Tentativa 1: importação normal
    print("Tentando importação normal...")
    from utils.utils_test import hello_spark, process_data
    print("Importação normal bem-sucedida!")
except ImportError as e1:
    print(f"Erro na importação normal: {e1}")
    
    try:
        # Tentativa 2: importação com caminho absoluto
        print("Tentando importação absoluta...")
        sys.path.append('/opt/spark/work-dir')
        from utils.utils_test import hello_spark, process_data
        print("Importação absoluta bem-sucedida!")
    except ImportError as e2:
        print(f"Erro na importação absoluta: {e2}")
        
        try:
            # Tentativa 3: importação manual do módulo
            print("Tentando importação manual...")
            import imp
            utils_test = imp.load_source('utils_test', '/opt/spark/work-dir/utils/utils_test.py')
            hello_spark = utils_test.hello_spark
            process_data = utils_test.process_data
            print("Importação manual bem-sucedida!")
        except Exception as e3:
            print(f"Erro na importação manual: {e3}")
            
            # Definir funções mock se tudo falhar
            print("Usando funções mock...")
            def hello_spark():
                return "Função mock - importação falhou"
                
            def process_data(df):
                return df.withColumn("mock_column", F.lit("Importação falhou"))

# Inicialização do Spark
print("="*80)
print("INICIALIZANDO SPARK")
spark = SparkSession.builder \
    .appName("MinioSparkTest") \
    .getOrCreate()

print(f"Versão do Spark: {spark.version}")

# Criar um DataFrame simples para teste
print("\nCriando DataFrame de teste...")
data = [
    ("Alice", 30),
    ("Bob", 25),
    ("Charlie", 35),
    ("David", 28)
]
columns = ["nome", "idade"]
df = spark.createDataFrame(data, columns)

# Mostrar amostra
df.show()

# Testar a função hello_spark
print("\nTestando função hello_spark:")
message = hello_spark()
print(message)

# Aplicar função process_data
print("\nAplicando função process_data:")
processed_df = process_data(df)
processed_df.show()

print("\nTESTE CONCLUÍDO COM SUCESSO!")
print("="*80)

# Encerrar a sessão Spark
spark.stop()



================================================================
End of Codebase
================================================================
