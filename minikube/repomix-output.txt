This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
access-control/
  crb-jupyter.yaml
  crb-spok.yaml
images_docker/
  datagenShadow/
    Dockerfile
    license.env
  jupyter-spark/
    Dockerfile
  spark-spok/
    Dockerfile
manifests/
  datagen/
    backup/
      shadowtraffic-config copy.yaml
      shadowtraffic-config.yaml
      shadowtraffic-config2.yaml
    generator-secrets.yaml
    shadowtraffic-config.yaml
    shadowtraffic-deployment.yaml
  deepstorage/
    minio.yaml
  jupyter-spark/
    jupyter-app.yaml
  management/
    reflector.yaml
  misc/
    access-control.yaml
    secrets.yaml
  processing/
    spark-operator.yaml
scripts-bash/
  test_automation_configk8.sh
  test_minio_connection.sh
  upload-script-spark.sh
secrets/
  minio-secrets.yaml
spark-jobs/
  spark-job-test-original-spark-imag.yaml
  spark-job-test.yaml
spark-src/
  test_spark.py
  test_spark2.py
license.env

================================================================
Files
================================================================

================
File: access-control/crb-jupyter.yaml
================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-jupyter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: default  
    namespace: jupyter

================
File: access-control/crb-spok.yaml
================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-spok
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: default  
    namespace: processing

================
File: images_docker/datagenShadow/Dockerfile
================
# Usar a imagem base do ShadowTraffic
FROM shadowtraffic/shadowtraffic:latest

# Instrução para usar variáveis de ambiente no ShadowTraffic
ENV LICENSE_ID=$LICENSE_ID
ENV LICENSE_EMAIL=$LICENSE_EMAIL
ENV LICENSE_ORGANIZATION=$LICENSE_ORGANIZATION
ENV LICENSE_EDITION=$LICENSE_EDITION
ENV LICENSE_EXPIRATION=$LICENSE_EXPIRATION
ENV LICENSE_SIGNATURE=$LICENSE_SIGNATURE
ENV AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
ENV AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
ENV AWS_REGION=$AWS_REGION

# Comando de entrada, garantindo que as variáveis sejam usadas
CMD ["shadowtraffic", "--config", "/workspace/v0_local_gen_minio.json"]

================
File: images_docker/datagenShadow/license.env
================
LICENSE_ID=cb9552fb-2902-49ce-b178-61190a602079
LICENSE_EMAIL=gabriel2024case@gmail.com
LICENSE_ORGANIZATION=gordostest
LICENSE_EDITION=ShadowTraffic Free Trial
LICENSE_EXPIRATION=2025-03-23
LICENSE_SIGNATURE=cI+afvP4GPbNgN95/rGY/lHNlyICrctVviPbP7AtqVpuwXufaNX7lg8cPfbgnsd6ljuUfRceGeYLANF7L5pJj1kQFc/mQhAAowxbXjhUCRcjh9T8iqW7P9Fc6AHHbCDJ0/niEQ2Qvl/98Knke13CvoFCzPnEZHCQVh9Sy5EcpFA8T9j5LYGbv+YxdKYxxT68aPL9lRSZPeLFwEVjY83YCoM4hlYyq9FTrYCKQwIE0UoJjhY8Uy1Gt5Iqj0z4MoFMyTOAph8AmA0iZdckTEZO6dxUu5HrFmuL4Y9EIUqjydFXYzqwtvvaH7i5w4fSCdJhhhDRM94cD5JIw+B5WykBug==

================
File: images_docker/jupyter-spark/Dockerfile
================
FROM jupyter/pyspark-notebook:latest

USER root

# Install additional Python packages
RUN pip install --no-cache-dir \
    delta-spark \
    boto3 \
    s3fs \
    matplotlib \
    seaborn \
    pandas \
    pyarrow

# Create directories for resources and configs
RUN mkdir -p /home/jovyan/resources /home/jovyan/config /home/jovyan/work && \
    chown -R jovyan:users /home/jovyan/resources /home/jovyan/config /home/jovyan/work

# Switch back to jovyan user
USER jovyan

# Set environment variables for Spark
ENV PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.1.0,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell"

# Define the startup command for JupyterLab - simpler like your original
CMD ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser"]

================
File: images_docker/spark-spok/Dockerfile
================
FROM apache/spark-py:v3.3.2

# Switch to user root para adicionar jars e ferramentas
USER root

# Criar diretório para a aplicação
RUN mkdir -p /app

# Adicionar os JARs necessários para S3/MinIO e Delta Lake
ADD https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar $SPARK_HOME/jars/
ADD https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar $SPARK_HOME/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.431/aws-java-sdk-bundle-1.12.431.jar $SPARK_HOME/jars/
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.4/hadoop-common-3.3.4.jar $SPARK_HOME/jars/
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar $SPARK_HOME/jars/

# Definir permissões corretas
RUN chmod 644 $SPARK_HOME/jars/*.jar

# Instalar dependências adicionais necessárias
RUN pip install --no-cache-dir \
    delta-spark \
    pandas \
    pyarrow \
    matplotlib \
    seaborn

# Configurar ambiente para acesso ao MinIO
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Definir diretório de trabalho
WORKDIR /app

# Voltar para o usuário do Spark
USER ${spark_uid}

================
File: manifests/datagen/backup/shadowtraffic-config copy.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadowtraffic-config
  namespace: datagen
data:
  v0_local_gen_minio.json: |
    {
      "generators": [
        {
          "connection": "s3",
          "bucket": "landing",
          "bucketConfigs": {
            "keyPrefix": "shadow-traffic-data/customers/",
            "format": "jsonl"
          },
          "data": {
            "id": { "_gen": "uuid" },
            "profile": {
              "first_name": { "_gen": "string", "expr": "#{Name.firstName}" },
              "last_name": { "_gen": "string", "expr": "#{Name.lastName}" },
              "full_name": { "_gen": "string", "expr": "#{Name.fullName}" },
              "age": { "_gen": "uniformDistribution", "bounds": [18, 80], "decimals": 0 },
              "email": { "_gen": "string", "expr": "#{Internet.emailAddress}" },
              "avatar": { "_gen": "string", "expr": "https://cloudflare-ipfs.com/ipfs/Qmd3W5DuhgHirLHGVixi6V76LhCkZUz6pnFt5AJBiyvHye/avatar/#{number.numberBetween '1','1000'}.jpg" }
            },
            "address": {
              "street": { "_gen": "string", "expr": "#{Address.streetAddress}" },
              "city": { "_gen": "string", "expr": "#{Address.city}" },
              "state": { "_gen": "string", "expr": "#{Address.stateAbbr}" },
              "zipcode": { "_gen": "string", "expr": "#{Address.zipCode}" },
              "country": { "_gen": "string", "expr": "#{Address.country}" },
              "coordinates": {
                "latitude": { "_gen": "uniformDistribution", "bounds": [-90, 90], "decimals": 6 },
                "longitude": { "_gen": "uniformDistribution", "bounds": [-180, 180], "decimals": 6 }
              }
            }
          },
          "localConfigs": {
            "maxEvents": 100,
            "throttleMs": 100
          }
        }
      ],
      "connections": {
        "s3": {
          "kind": "s3",
          "connectionConfigs": {
            "endpoint": "http://minio.deepstorage.svc.cluster.local:9000",
            "pathStyleAccess": true,
            "forcePathStyle": true
          },
          "batchConfigs": {
            "lingerMs": 5000,
            "batchElements": 15,
            "batchBytes": 1048576
          }
        }
      }
    }

================
File: manifests/datagen/backup/shadowtraffic-config.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadowtraffic-config
  namespace: datagen
data:
  v0_local_gen_minio.json: |
    {
      "generators": [
        {
          "connection": "s3",
          "bucket": "landing",
          "bucketConfigs": {
            "keyPrefix": "shadow-traffic-data/",
            "format": "json"
          },
          "data": {
            "id": { "_gen": "uuid" },
            "name": { "_gen": "string", "expr": "#{Name.fullName}" },
            "age": { "_gen": "uniformDistribution", "bounds": [18, 80] }
          },
          "localConfigs": {
            "maxEvents": 100,
            "throttleMs": 1000
          }
        }
      ],
      "connections": {
        "s3": {
          "kind": "s3",
          "connectionConfigs": {
            "endpoint": "http://minio.deepstorage.svc.cluster.local:9000",
            "pathStyleAccess": true,
            "forcePathStyle": true
          }
        }
      }
    }

================
File: manifests/datagen/backup/shadowtraffic-config2.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadowtraffic-config
  namespace: datagen
data:
  v0_local_gen_minio.json: |
    {
      "generators": [
        {
          "connection": "s3",
          "bucket": "landing",
          "bucketConfigs": {
            "keyPrefix": "shadow-traffic-data/customers/",
            "format": "json"
          },
          "data": {
            "id": { "_gen": "uuid" },
            "profile": {
              "first_name": { "_gen": "string", "expr": "#{Name.firstName}" },
              "last_name": { "_gen": "string", "expr": "#{Name.lastName}" },
              "full_name": { "_gen": "string", "expr": "#{Name.fullName}" },
              "age": { "_gen": "uniformDistribution", "bounds": [18, 80], "decimals": 0 },
              "email": { "_gen": "string", "expr": "#{Internet.emailAddress}" },
              "avatar": { "_gen": "string", "expr": "https://cloudflare-ipfs.com/ipfs/Qmd3W5DuhgHirLHGVixi6V76LhCkZUz6pnFt5AJBiyvHye/avatar/#{number.numberBetween '1','1000'}.jpg" }
            },
            "address": {
              "street": { "_gen": "string", "expr": "#{Address.streetAddress}" },
              "city": { "_gen": "string", "expr": "#{Address.city}" },
              "state": { "_gen": "string", "expr": "#{Address.stateAbbr}" },
              "zipcode": { "_gen": "string", "expr": "#{Address.zipCode}" },
              "country": { "_gen": "string", "expr": "#{Address.country}" },
              "coordinates": {
                "latitude": { "_gen": "uniformDistribution", "bounds": [-90, 90], "decimals": 6 },
                "longitude": { "_gen": "uniformDistribution", "bounds": [-180, 180], "decimals": 6 }
              }
            }
          },
          "localConfigs": {
            "maxEvents": 100,
            "throttleMs": 1000
          }
        }
      ],
      "connections": {
        "s3": {
          "kind": "s3",
          "connectionConfigs": {
            "endpoint": "http://minio.deepstorage.svc.cluster.local:9000",
            "pathStyleAccess": true,
            "forcePathStyle": true
          }
        }
      }
    }

================
File: manifests/datagen/generator-secrets.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: generator-secrets
  namespace: datagen
  annotations:
    reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "deepstorage,datagen"
type: Opaque
stringData:
  LICENSE_ID: "cb9552fb-2902-49ce-b178-61190a602079"
  LICENSE_EMAIL: "gabriel2024case@gmail.com"
  LICENSE_ORGANIZATION: "gordostest"
  LICENSE_EDITION: "ShadowTraffic Free Trial"
  LICENSE_EXPIRATION: "2025-03-23"
  LICENSE_SIGNATURE: "cI+afvP4GPbNgN95/rGY/lHNlyICrctVviPbP7AtqVpuwXufaNX7lg8cPfbgnsd6ljuUfRceGeYLANF7L5pJj1kQFc/mQhAAowxbXjhUCRcjh9T8iqW7P9Fc6AHHbCDJ0/niEQ2Qvl/98Knke13CvoFCzPnEZHCQVh9Sy5EcpFA8T9j5LYGbv+YxdKYxxT68aPL9lRSZPeLFwEVjY83YCoM4hlYyq9FTrYCKQwIE0UoJjhY8Uy1Gt5Iqj0z4MoFMyTOAph8AmA0iZdckTEZO6dxUu5HrFmuL4Y9EIUqjydFXYzqwtvvaH7i5w4fSCdJhhhDRM94cD5JIw+B5WykBug=="

================
File: manifests/datagen/shadowtraffic-config.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadowtraffic-config
  namespace: datagen
data:
  v0_local_gen_minio.json: |
    {
      "generators": [
        {
          "connection": "s3",
          "bucket": "landing",
          "bucketConfigs": {
            "keyPrefix": "shadow-traffic-data/customers/",
            "format": "json"
          },
          "fork": {
            "maxForks": 20,
            "key": { "_gen": "uuid" }
          },
          "data": {
            "id": { "_gen": "var", "var": "forkKey" },
            "profile": {
              "first_name": { "_gen": "string", "expr": "#{Name.firstName}" },
              "last_name": { "_gen": "string", "expr": "#{Name.lastName}" },
              "full_name": { "_gen": "string", "expr": "#{Name.fullName}" },
              "age": { "_gen": "uniformDistribution", "bounds": [18, 80], "decimals": 0 },
              "email": { "_gen": "string", "expr": "#{Internet.emailAddress}" },
              "avatar": { "_gen": "string", "expr": "https://cloudflare-ipfs.com/ipfs/Qmd3W5DuhgHirLHGVixi6V76LhCkZUz6pnFt5AJBiyvHye/avatar/#{number.numberBetween '1','1000'}.jpg" }
            },
            "address": {
              "street": { "_gen": "string", "expr": "#{Address.streetAddress}" },
              "city": { "_gen": "string", "expr": "#{Address.city}" },
              "state": { "_gen": "string", "expr": "#{Address.stateAbbr}" },
              "zipcode": { "_gen": "string", "expr": "#{Address.zipCode}" },
              "country": { "_gen": "string", "expr": "#{Address.country}" },
              "coordinates": {
                "latitude": { "_gen": "uniformDistribution", "bounds": [-90, 90], "decimals": 6 },
                "longitude": { "_gen": "uniformDistribution", "bounds": [-180, 180], "decimals": 6 }
              }
            }
          },
          "localConfigs": {
            "maxEvents": 5,
            "throttleMs": 1000
          }
        }
      ],
      "connections": {
        "s3": {
          "kind": "s3",
          "connectionConfigs": {
            "endpoint": "http://minio.deepstorage.svc.cluster.local:9000",
            "pathStyleAccess": true,
            "forcePathStyle": true
          }
        }
      }
    }

================
File: manifests/datagen/shadowtraffic-deployment.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shadowtraffic
  namespace: datagen
spec:
  replicas: 1
  selector:
    matchLabels:
      app: shadowtraffic
  template:
    metadata:
      labels:
        app: shadowtraffic
    spec:
      containers:
        - name: shadowtraffic
          image: shadowtraffic-datagen:latest
          imagePullPolicy: Never
          env:
            - name: LICENSE_ID
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_ID
            - name: LICENSE_EMAIL
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EMAIL
            - name: LICENSE_ORGANIZATION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_ORGANIZATION
            - name: LICENSE_EDITION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EDITION
            - name: LICENSE_EXPIRATION
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_EXPIRATION
            - name: LICENSE_SIGNATURE
              valueFrom:
                secretKeyRef:
                  name: generator-secrets
                  key: LICENSE_SIGNATURE
            - name: AWS_ACCESS_KEY_ID
              value: "miniouser"
            - name: AWS_SECRET_ACCESS_KEY
              value: "miniosecret" 
            - name: AWS_REGION
              value: "us-east-1"
          volumeMounts:
            - name: config-volume
              mountPath: /workspace/v0_local_gen_minio.json
              subPath: v0_local_gen_minio.json
      volumes:
        - name: config-volume
          configMap:
            name: shadowtraffic-config

================
File: manifests/deepstorage/minio.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: minio
  annotations:
    argocd.argoproj.io/sync-wave: "6"
  namespace: cicd
spec:
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: deepstorage
  project: default
  source:
    repoURL: 'https://charts.bitnami.com/bitnami'
    targetRevision: 14.0.0
    helm:
      values: |-
        auth:
          existingSecret: minio-secrets
        defaultBuckets: "landing, lakehouse, scripts"
        service:
          type: LoadBalancer
        persistence:
          size: 10Gi
        volumePermissions:
          enabled: true
    chart: minio
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/jupyter-spark/jupyter-app.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: custom-jupyter
  namespace: cicd
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: jupyter
  source:
    repoURL: git@github.com:Gabriel-Philot/shadow-traffic-studies.git
    targetRevision: HEAD
    path: minikube/manifests/jupyter-spark
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
---
apiVersion: v1
kind: Service
metadata:
  name: custom-jupyter
  namespace: jupyter
spec:
  type: LoadBalancer
  ports:
  - port: 8888
    targetPort: 8888
    protocol: TCP
  selector:
    app: custom-jupyter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-jupyter
  namespace: jupyter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: custom-jupyter
  template:
    metadata:
      labels:
        app: custom-jupyter
    spec:
      containers:
      - name: jupyter
        image: jupyter:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 8888
        env:
        - name: MINIO_ENDPOINT
          value: "http://minio.deepstorage.svc.cluster.local:9000"
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secrets
              key: root-user
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secrets
              key: root-password
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
          requests:
            memory: "2Gi"
            cpu: "1"

================
File: manifests/management/reflector.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: reflector
  annotations:
    argocd.argoproj.io/sync-wave: "5"
  namespace: cicd
spec:
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: management
  project: default
  source:
    repoURL: 'https://emberstack.github.io/helm-charts'
    targetRevision: 7.0.151        
    chart: reflector
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/misc/access-control.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: access-control
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: misc
  project: default
  source:
    repoURL: "git@github.com:Gabriel-Philot/shadow-traffic-studies.git"
    path: minikube/access-control/
    targetRevision: HEAD
    directory:
      recurse: true
      jsonnet: {}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/misc/secrets.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: secrets
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: misc
  project: default
  source:
    repoURL: "git@github.com:Gabriel-Philot/shadow-traffic-studies.git"
    path: minikube/secrets/
    targetRevision: HEAD
    directory:
      recurse: true
      jsonnet: {}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: manifests/processing/spark-operator.yaml
================
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: spark
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  namespace: cicd
spec:
  destination:
    server: "https://kubernetes.default.svc"
    namespace: processing
  project: default
  source:
    repoURL: "https://kubeflow.github.io/spark-operator"
    targetRevision: 1.4.2
    helm:
      values: |-
        webhook:
          enable: true
    chart: spark-operator
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

================
File: scripts-bash/test_automation_configk8.sh
================
#!/bin/bash

# Exit on error
set -e

# Color codes for better output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
print_section() {
    echo -e "${BLUE}===================================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}===================================================${NC}"
}

print_success() {
    echo -e "${GREEN}✓ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}

print_error() {
    echo -e "${RED}✗ $1${NC}"
}

wait_for_pod_ready() {
    namespace=$1
    label=$2
    echo -e "Waiting for pods with label $label in namespace $namespace to be ready..."
    
    while [[ $(kubectl get pods -n $namespace -l $label -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do
        echo -n "."
        sleep 3
    done
    echo
    print_success "Pod is ready!"
}

wait_for_loadbalancer_ip() {
    namespace=$1
    label=$2
    echo -e "Waiting for LoadBalancer IP for service with label $label in namespace $namespace..."
    
    while [[ -z $(kubectl get services -n $namespace -l $label -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}") ]]; do
        echo -n "."
        sleep 3
    done
    echo
    print_success "LoadBalancer IP is ready!"
}

# Assuming minikube is already running
print_success "Minikube is assumed to be running with tunnel active"

print_section "Creating necessary namespaces"
kubectl create namespace orchestrator 2>/dev/null || echo "Namespace orchestrator already exists"
kubectl create namespace deepstorage 2>/dev/null || echo "Namespace deepstorage already exists"
kubectl create namespace cicd 2>/dev/null || echo "Namespace cicd already exists"
kubectl create namespace app 2>/dev/null || echo "Namespace app already exists"
kubectl create namespace management 2>/dev/null || echo "Namespace management already exists"
kubectl create namespace misc 2>/dev/null || echo "Namespace misc already exists"
kubectl create namespace jupyter 2>/dev/null || echo "Namespace jupyter already exists"
kubectl create namespace processing 2>/dev/null || echo "Namespace jupyter already exists"
kubectl create namespace datagen 2>/dev/null || echo "Namespace datagen already exists"

print_section "Installing ArgoCD"
echo "Installing ArgoCD with Helm..."
helm install argocd argo/argo-cd --namespace cicd --version 5.27.1 || echo "ArgoCD already installed or error occurred"

echo "Patching ArgoCD server service to use LoadBalancer..."
kubectl patch svc argocd-server -n cicd -p '{"spec": {"type": "LoadBalancer"}}'

print_section "Waiting for ArgoCD to be ready"
wait_for_pod_ready "cicd" "app.kubernetes.io/name=argocd-server"
wait_for_loadbalancer_ip "cicd" "app.kubernetes.io/name=argocd-server"

# Store ArgoCD LoadBalancer IP
ARGOCD_LB=$(kubectl get services -n cicd -l app.kubernetes.io/name=argocd-server -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}")
print_success "ArgoCD LoadBalancer IP: $ARGOCD_LB"

# Get ArgoCD admin password
ARGOCD_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -n cicd -o jsonpath="{.data.password}" | base64 -d)
print_success "ArgoCD admin password: $ARGOCD_PASSWORD"

echo "ArgoCD web interface available at: http://$ARGOCD_LB"

# Assuming minikube tunnel is already running
print_success "Minikube tunnel is assumed to be running in another terminal"

print_section "Adding Git repository to ArgoCD"
echo "Logging into ArgoCD..."
kubectl get secret argocd-initial-admin-secret -n cicd -o jsonpath="{.data.password}" | base64 -d | xargs -t -I {} argocd login $ARGOCD_LB --username admin --password {} --insecure

echo "Adding repository to ArgoCD..."
argocd repo add git@github.com:Gabriel-Philot/shadow-traffic-studies.git --ssh-private-key-path ~/.ssh/id_ed25519 --insecure-skip-server-verification

print_success "Repository added to ArgoCD"

print_section "Setting up Reflector for secret management"
kubectl apply -f manifests/management/reflector.yaml

print_warning "Waiting for Reflector to be deployed (30 seconds)..."
sleep 30

print_section "Setting up Secrets"
kubectl apply -f manifests/misc/secrets.yaml

print_warning "Waiting for Secrets to be processed (10 seconds)..."
sleep 10

print_section "Setting up Acess control"
kubectl apply -f access-control/crb-jupyter.yaml

print_warning "Waiting for Acess control to be processed (10 seconds)..."
sleep 10

print_section "Setting up MinIO Storage"
kubectl apply -f manifests/deepstorage/minio.yaml

print_warning "Waiting for MinIO to be deployed (45 seconds)..."
sleep 45

print_section "Building and deploying ShadowTraffic data generator"

# Switch to minikube's Docker environment
eval $(minikube docker-env)

echo "Building ShadowTraffic Docker image..."
docker build --no-cache -f images_docker/datagenShadow/Dockerfile -t shadowtraffic-datagen:latest .

echo "Deploying ShadowTraffic components..."
kubectl apply -f manifests/datagen/generator-secrets.yaml
kubectl apply -f manifests/datagen/shadowtraffic-config.yaml
kubectl apply -f manifests/datagen/shadowtraffic-deployment.yaml

wait_for_pod_ready "datagen" "app=shadowtraffic"

print_section "Getting access credentials"

# Get MinIO LoadBalancer IP
MINIO_LB=$(kubectl get services -n deepstorage -l app.kubernetes.io/name=minio -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}")
MINIO_USER=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-user}" | base64 -d)
MINIO_PASSWORD=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-password}" | base64 -d)

print_success "Setup Complete!"
echo "================ SERVICE ACCESS INFORMATION ================"
echo "ArgoCD:"
echo "  URL: http://$ARGOCD_LB"
echo "  Username: admin"
echo "  Password: $ARGOCD_PASSWORD"
echo ""
echo "MinIO:"
echo "  URL: http://$MINIO_LB:9000"
echo "  Username: $MINIO_USER"
echo "  Password: $MINIO_PASSWORD"

print_success "Git repository has been configured in ArgoCD"

echo "To check the ShadowTraffic generator status, run:"
echo "kubectl get pods -n datagen"

# Return to the original Docker environment
eval $(minikube docker-env -u)

================
File: scripts-bash/test_minio_connection.sh
================
#!/bin/bash

# Install MinIO client if not already installed
if ! command -v mc &> /dev/null; then
    echo "Installing MinIO client..."
    wget https://dl.min.io/client/mc/release/linux-amd64/mc
    chmod +x mc
    sudo mv mc /usr/local/bin/
fi

# Configure MinIO client
mc alias set minikube-minio http://10.108.217.62:9000 miniouser miniosecret

# List buckets to verify connection
echo "Testing connection to MinIO..."
mc ls minikube-minio

# Check if landing bucket exists, create if it doesn't
if ! mc ls minikube-minio/landing &> /dev/null; then
    echo "Creating landing bucket..."
    mc mb minikube-minio/landing
fi

echo "Connection test complete!"

================
File: scripts-bash/upload-script-spark.sh
================
#!/bin/bash

# Cores para saída formatada
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
NC='\033[0m' # Sem cor

echo -e "${YELLOW}Iniciando upload do script para o MinIO...${NC}"

# Definir variáveis
MINIO_ENDPOINT=$(kubectl get services -n deepstorage -l app.kubernetes.io/name=minio -o jsonpath="{.items[0].status.loadBalancer.ingress[0].ip}"):9000
MINIO_USER=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-user}" | base64 -d)
MINIO_PASSWORD=$(kubectl get secret minio-secrets -n deepstorage -o jsonpath="{.data.root-password}" | base64 -d)

echo -e "MinIO endpoint: ${MINIO_ENDPOINT}"
echo -e "MinIO user: ${MINIO_USER}"

# Verificar se a AWS CLI está instalada
if ! command -v aws &> /dev/null; then
    echo -e "${YELLOW}AWS CLI não encontrada. Instalando...${NC}"
    pip install awscli
fi

# Configurar AWS CLI para MinIO
echo -e "Configurando AWS CLI para acessar MinIO..."
mkdir -p ~/.aws

cat > ~/.aws/credentials << EOF
[default]
aws_access_key_id = ${MINIO_USER}
aws_secret_access_key = ${MINIO_PASSWORD}
EOF

cat > ~/.aws/config << EOF
[default]
region = us-east-1
EOF

# Verificar se o bucket scripts existe
echo -e "Verificando se o bucket 'scripts' existe..."
if ! aws --endpoint-url=http://${MINIO_ENDPOINT} s3 ls s3://scripts &> /dev/null; then
    echo -e "Bucket 'scripts' não existe. Criando..."
    aws --endpoint-url=http://${MINIO_ENDPOINT} s3 mb s3://scripts
    echo -e "${GREEN}Bucket 'scripts' criado com sucesso!${NC}"
else
    echo -e "${GREEN}Bucket 'scripts' já existe.${NC}"
fi

# Fazer upload do script de teste
echo -e "\n${YELLOW}Fazendo upload do script de teste para o MinIO...${NC}"
aws --endpoint-url=http://${MINIO_ENDPOINT} s3 cp spark-src/test_spark2.py s3://scripts/minio-spark-scripts/

if [ $? -eq 0 ]; then
    echo -e "${GREEN}Script enviado para o MinIO com sucesso!${NC}"
    echo -e "Caminho no MinIO: s3a://scripts/minio-spark-scripts/test_spark2.py"
else
    echo -e "${RED}Erro ao enviar script para o MinIO!${NC}"
fi

echo -e "\n${GREEN}Processo de upload concluído!${NC}"

================
File: secrets/minio-secrets.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: minio-secrets
  namespace: deepstorage
  annotations:
    reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
    reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "deepstorage,datagen,jupyter, processing"
type: Opaque
data:
  root-user: "bWluaW91c2Vy"
  root-password: "bWluaW9zZWNyZXQ="

================
File: spark-jobs/spark-job-test-original-spark-imag.yaml
================
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: minio-spark-teste
  namespace: processing
  labels:
    app: spark
    product: data-processing
    source: minio
    table: teste
    stage: dev
spec:
  type: Python
  mode: cluster
  image: spark:3.5.0
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "s3a://scripts/minio-spark-scripts/test_spark2.py"
  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
  driver:
    cores: 1
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  executor:
    cores: 1
    instances: 2
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  deps:
    packages:
      - org.apache.hadoop:hadoop-aws:3.3.4
      - org.apache.hadoop:hadoop-common:3.3.4
      - io.delta:delta-spark_2.12:3.2.0
      - com.amazonaws:aws-java-sdk-bundle:1.11.901
    repositories:
      - https://repo1.maven.org/maven2
  sparkConf:
    "spark.hadoop.fs.s3a.endpoint": "http://minio.deepstorage.svc.cluster.local:9000"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.delta.logStore.class": "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore"
    "spark.databricks.delta.schema.autoMerge.enabled": "true"
    "spark.databricks.delta.retentionDurationCheck.enabled": "false"
    "spark.jars.ivy": "/tmp/.ivy2"

================
File: spark-jobs/spark-job-test.yaml
================
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: minio-spark-teste
  namespace: processing
  labels:
    app: spark
    product: data-processing
    source: minio
    table: teste
    stage: dev
spec:
  type: Python
  mode: cluster
  image: "minio-spark-dev:latest"
  imagePullPolicy: Never
  mainApplicationFile: "s3a://scripts/minio-spark-scripts/test_spark.py"
  sparkVersion: "3.3.2"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
  driver:
    cores: 1
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  executor:
    cores: 1
    instances: 2
    memory: "2G"
    serviceAccount: default
    envSecretKeyRefs:
      AWS_ACCESS_KEY_ID:
        name: minio-secrets
        key: root-user
      AWS_SECRET_ACCESS_KEY:
        name: minio-secrets
        key: root-password
  sparkConf:
    "spark.hadoop.fs.s3a.endpoint": "http://minio.deepstorage.svc.cluster.local:9000"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"

================
File: spark-src/test_spark.py
================
"""
Script de teste para verificar o acesso direto ao MinIO via S3A
Este arquivo deve ser carregado em: s3a://scripts/minio-spark-scripts/teste_spark.py
"""
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os
import socket
import datetime

# Inicializar o Spark com as configurações para Delta Lake
spark = SparkSession.builder \
    .appName("MinioSparkTest") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

print("="*80)
print("TESTE DE ACESSO DIRETO AO SCRIPT NO MINIO")
print("="*80)

# Imprimir informações do ambiente para diagnóstico
print(f"Data/Hora atual: {datetime.datetime.now()}")
print(f"Hostname: {socket.gethostname()}")
print(f"Diretório atual: {os.getcwd()}")
print(f"Versão do Spark: {spark.version}")

# Criar um DataFrame simples para teste
print("\nCriando DataFrame de teste...")
data = [
    ("Alice", 30),
    ("Bob", 25),
    ("Charlie", 35),
    ("David", 28)
]
columns = ["nome", "idade"]
df = spark.createDataFrame(data, columns)

# Mostrar amostra
df.show()

# Aplicar algumas transformações
df_transformed = df.withColumn("mensagem", 
                             F.concat(F.lit("Olá "), 
                                     F.col("nome"), 
                                     F.lit("! Você tem "), 
                                     F.col("idade").cast("string"), 
                                     F.lit(" anos.")))
print("DataFrame transformado:")
df_transformed.show(truncate=False)

# Tentar listar arquivos no MinIO
print("\nTestando acesso ao MinIO...")
try:
    # Listar conteúdo do bucket landing
    landing_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(
        spark._jsc.hadoopConfiguration()
    ).listStatus(spark._jvm.org.apache.hadoop.fs.Path("s3a://landing/"))
    
    print("Arquivos no bucket landing:")
    for file_status in landing_files:
        print(f"  - {file_status.getPath().getName()}")

    # Tentar acessar dados reais se existirem
    try:
        # Tentar ler arquivo JSON do bucket landing
        customer_df = spark.read.format("json").load("s3a://landing/shadow-traffic-data/customers/")
        print("\nDados de clientes encontrados! Amostra:")
        customer_df.printSchema()
        customer_df.show(5, truncate=False)
        print(f"Total de registros: {customer_df.count()}")
    except Exception as e:
        print(f"Não foi possível ler dados de clientes: {e}")
        
except Exception as e:
    print(f"Erro ao acessar MinIO: {str(e)}")

# Escrever resultado em Delta format como teste
try:
    print("\nTentando escrever dados no formato Delta...")
    df_transformed.write \
        .format("delta") \
        .mode("overwrite") \
        .save("s3a://scripts/teste-output/delta-teste")
    print("Dados escritos com sucesso no formato Delta!")
except Exception as e:
    print(f"Erro ao escrever no formato Delta: {str(e)}")

print("\nTESTE CONCLUÍDO COM SUCESSO!")
print("="*80)

# Encerrar a sessão Spark
spark.stop()

================
File: spark-src/test_spark2.py
================
"""
Script de teste para verificar o acesso direto ao MinIO via S3A
"""
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os
import socket
import datetime

# Inicialização simplificada - as configurações vêm do SparkApplication
spark = SparkSession.builder \
    .appName("MinioSparkTest") \
    .getOrCreate()

print("="*80)
print("TESTE DE ACESSO DIRETO AO SCRIPT NO MINIO")
print("="*80)

# Imprimir informações do ambiente para diagnóstico
print(f"Data/Hora atual: {datetime.datetime.now()}")
print(f"Hostname: {socket.gethostname()}")
print(f"Diretório atual: {os.getcwd()}")
print(f"Versão do Spark: {spark.version}")

# Resto do código mantido igual...

================
File: license.env
================
LICENSE_ID=cb9552fb-2902-49ce-b178-61190a602079
LICENSE_EMAIL=gabriel2024case@gmail.com
LICENSE_ORGANIZATION=gordostest
LICENSE_EDITION=ShadowTraffic Free Trial
LICENSE_EXPIRATION=2025-03-23
LICENSE_SIGNATURE=cI+afvP4GPbNgN95/rGY/lHNlyICrctVviPbP7AtqVpuwXufaNX7lg8cPfbgnsd6ljuUfRceGeYLANF7L5pJj1kQFc/mQhAAowxbXjhUCRcjh9T8iqW7P9Fc6AHHbCDJ0/niEQ2Qvl/98Knke13CvoFCzPnEZHCQVh9Sy5EcpFA8T9j5LYGbv+YxdKYxxT68aPL9lRSZPeLFwEVjY83YCoM4hlYyq9FTrYCKQwIE0UoJjhY8Uy1Gt5Iqj0z4MoFMyTOAph8AmA0iZdckTEZO6dxUu5HrFmuL4Y9EIUqjydFXYzqwtvvaH7i5w4fSCdJhhhDRM94cD5JIw+B5WykBug==



================================================================
End of Codebase
================================================================
